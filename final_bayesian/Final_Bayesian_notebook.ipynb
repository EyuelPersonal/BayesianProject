{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(\n",
    "    'AAPL', \n",
    "    '2018-06-29', '2023-06-09', interval='1d')\n",
    "df_og= df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'].plot(figsize=(15, 5), title='AAPL Stock Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_returns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.returns.plot(figsize=(15, 5), title='AAPL % Returns', xlabel='Returns' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['moving_average'] = df['returns'].rolling(10, closed='left').mean()\n",
    "\n",
    "# volatility std * root(T)\n",
    "df['volatility'] = df['returns'].rolling(10, closed='left').std() * np.sqrt(10) * 1000\n",
    "\n",
    "# in the previous line of coe we multiplied by 1000 to scale the data to make it easier to work with\n",
    "# this allowed us to interpret the histogram plot easier and to set our prior distributions\n",
    "# we rescaled it back to the original scale later on to make it easier to interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.volatility.plot(figsize=(15, 5), title='AAPL Volatility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na rows\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['returns', 'volatility']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_volatility_cluster(df, mus=[20,60,100], sigmas=[10,10,10]):\n",
    "    \"\"\"\n",
    "    Assigns a volatility cluster to each value in the DataFrame.\n",
    "    Give a df with a column named 'Value', this should be the volatility measure of a given window. \n",
    "    Let the index be the window or any arbitrary index.\n",
    "    \"\"\"\n",
    "    # df drop rows with nan\n",
    "    df.dropna(inplace=True)\n",
    "    # Prepare the data\n",
    "    \n",
    "    values = df['volatility'].values\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # Specify the number of clusters\n",
    "        k = 3\n",
    "        # Priors for the cluster parameters\n",
    "        mus = pm.Normal('mus', mu=mus, sd=10, shape=k)\n",
    "        sigmas = pm.HalfNormal('sigmas', sd=10, shape=k)\n",
    "        weights = pm.Dirichlet('weights', a=np.ones(k))\n",
    "\n",
    "        # Likelihood\n",
    "        likelihood = pm.NormalMixture('likelihood', w=weights, mu=mus, sd=sigmas, observed=values)\n",
    "\n",
    "        # Sample from the posterior\n",
    "        trace = pm.sample(2000, tune=1000)\n",
    "        \n",
    "    cluster_means = np.array(trace['mus'][-1])\n",
    "    diff = cluster_means - df['volatility'].values[:, np.newaxis]\n",
    "    cluster = np.argmin(np.abs(diff), axis=1)\n",
    "    df['cluster'] = cluster\n",
    "\n",
    "\n",
    "\n",
    "    return df, trace\n",
    "\n",
    "    # Extract the cluster assignments\n",
    "\n",
    "df_clusters, trace = assign_volatility_cluster(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clusters.value_counts('cluster')\n",
    "df_clusters.value_counts('cluster') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join df_clusters and df_og\n",
    "df_clusters = df_clusters.join(df_og, how='left')\n",
    "df_clusters.reset_index(inplace=True)\n",
    "df_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mcolors\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "# Get unique categories and assign corresponding color map\n",
    "unique_categories = sorted(df_clusters['cluster'].unique())  # Sort the unique categories\n",
    "color_map = mcolors.ListedColormap(['#90bcdc', '#ffbc84', '#98cc94'])  # Define specific colors for the categories\n",
    "\n",
    "# Plot scatter points with color based on cluster\n",
    "norm = mcolors.Normalize(vmin=0, vmax=len(unique_categories)-1)\n",
    "plt.scatter(df_clusters.index, df_clusters['High'], c=df_clusters['cluster'].astype('category').cat.codes,\n",
    "            cmap=color_map, norm=norm)\n",
    "\n",
    "# Create a legend with matching colors\n",
    "legend_labels = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map(norm(i)), markersize=8)\n",
    "                 for i, _ in enumerate(unique_categories)]\n",
    "plt.legend(legend_labels, unique_categories)\n",
    "\n",
    "# Set labels for x and y axes\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('AAPL Stock Price with Clusters labels')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "df['cluster'] = label_encoder.fit_transform(df['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cluster_names = df.cluster.unique()\n",
    "train_cluster_idx = df.cluster.values\n",
    "\n",
    "n_train_types = len(df.cluster.unique())\n",
    "df.volatility = df.volatility/1000\n",
    "\n",
    "df[['cluster', 'volatility']].head()\n",
    "\n",
    "#plot histogram of volatility\n",
    "plt.title('Histogram of Volatility of AAPL Stock')\n",
    "plt.hist(df.volatility, bins=30, edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category1_values = df[df.cluster == 0].volatility.values\n",
    "category2_values = df[df.cluster == 1].volatility.values\n",
    "category3_values = df[df.cluster == 2].volatility.values\n",
    "\n",
    "category1_width=category1_values.max() - category1_values.min()\n",
    "category2_width=category2_values.max() - category2_values.min()\n",
    "category3_width=category3_values.max() - category3_values.min()\n",
    "bin_width = 0.005\n",
    "bins_1 = category1_width / bin_width\n",
    "bins_2 = category2_width / bin_width\n",
    "bins_3 = category3_width / bin_width\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.hist(category1_values, bins=int(round(bins_1)),  label='Cluster 0', color='#90bcdc')\n",
    "plt.hist(category2_values, bins=int(round(bins_2)),  label='Cluster 1', color='#ffbc84')\n",
    "plt.hist(category3_values, bins=int(round(bins_3)),  label='Cluster 2', color='#98cc94')\n",
    "\n",
    "plt.xlabel('volatility')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histograms by Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_model:\n",
    "    # global model parameters\n",
    "    α_μ_tmp = pm.Normal('α_μ_tmp', mu=0.5, sd=0.1)\n",
    "    α_σ_tmp = pm.HalfNormal('α_σ_tmp', .05)\n",
    "    β_μ = pm.Normal('β_μ', mu=0.5, sd=0.1)\n",
    "    β_σ = pm.HalfNormal('β_σ', .05)\n",
    "\n",
    "    # Cluster specific model parameters\n",
    "    α_tmp = pm.Normal('α_tmp', mu=α_μ_tmp, sd=α_σ_tmp, shape=3)  \n",
    "    # Intercept for each Cluster, distributed around Cluster mean \n",
    "    β = pm.Normal('β', mu=β_μ, sd=β_σ, shape=3)\n",
    "    # Model error\n",
    "    eps = pm.HalfCauchy('eps', .5)\n",
    "\n",
    "    return_est = α_tmp[train_cluster_idx] + β[train_cluster_idx]\n",
    "\n",
    "    # Data likelihood\n",
    "    return_like = pm.Normal('return_like', mu=return_est, sd=eps, observed=df.volatility)\n",
    "    \n",
    "with hierarchical_model:\n",
    "    hierarchical_trace = pm.sample(2000, tune=1000, target_accept=.9)\n",
    "    \n",
    "pm.traceplot(hierarchical_trace, var_names=['α_μ_tmp', 'β_μ', 'α_σ_tmp', 'β_σ', 'eps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(hierarchical_trace, var_names=['α_tmp', 'β'], combined=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc = pm.sample_posterior_predictive(hierarchical_trace, samples=2000, model=hierarchical_model)\n",
    "az.r2_score(df.returns.values, ppc['return_like'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningToBranch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
